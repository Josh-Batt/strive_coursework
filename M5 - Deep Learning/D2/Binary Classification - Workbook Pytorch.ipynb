{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "    <h2 align=\"center\">Deep Learning Fundamentals</h2>\n",
    "    <h2 align=\"center\" style=\"color:#01ff84\">Binary Clasification: Pytorch</h2>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hjwSKXAO6w2j"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQVubjP48NOr"
   },
   "source": [
    "Till now you have worked with numpy to create and manage arrays. However, Pytorch has its own way to define arrays, or tensors in general, that are more convenient for computational efficiency:\n",
    "\n",
    "The torch.tensor command has the same purpose of the np.array, but in PyTorch everything is a Tensor as opposed to a vector or matrix. We define types in PyTorch using the dtype=torch.xxx command.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EbW3fZBQ8F_f"
   },
   "outputs": [],
   "source": [
    "X = torch.tensor(([2, 9], [1, 5], [3, 6]), dtype=torch.float) # 3 X 2 tensor\n",
    "y = torch.tensor(([92], [100], [89]), dtype=torch.float) # 3 X 1 tensor\n",
    "test_sample = torch.tensor(([4, 8]), dtype=torch.float) # 1 X 2 tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojuRtlUO9f2A"
   },
   "source": [
    "You can inspect the size of the tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aByDVb-U9X8e",
    "outputId": "63978641-9d14-4c8d-ccfe-9374b6a0430f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X.size())\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ysRjgTe9_1w"
   },
   "source": [
    "If you already have a numpy array that you want to convert in tensor you can use:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41-_f5mY9hMP",
    "outputId": "10e9e305-e628-4994-9e44-0a6474612d0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 10])\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "numpy_array = np.array(np.random.rand(1000,10))\n",
    "pytorch_tensor = torch.from_numpy(numpy_array)\n",
    "print(pytorch_tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xxuR29T-Z5r"
   },
   "source": [
    "Pytorch has the advantage to be much more customizable and Keras, but this means that you have to manually define more things. \n",
    "\n",
    "Indeed, a neural network in Pytorch is a subclass of the nn.Module parent class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AkK6ULdl-Oiz"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self ):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otF4dBqSCpFn"
   },
   "source": [
    "The code above is how you define a neural network class using Pytorch. You can add some arguments to specify the input and the output sizes, or the number of neurons in the hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4EhGXDx-_VMD"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_hidden):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKevxbyRHe52"
   },
   "source": [
    "The fully-connected layer in Pytorch (equivalent to the Dense layer in Keras) is given by the function nn.Linear() that takes as input the input shape and the number of neurons. We can define the layers as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WkvYry5987U8"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_hidden):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, num_hidden)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.linear2 = nn.Linear(num_hidden, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBoLZeTJPy2A"
   },
   "source": [
    "We can instantiate a neural network as we do for a class (in the following example I will pass 10 as input_dim and 5 neurons for the hidden layer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YiY6s5x5-Bgp"
   },
   "outputs": [],
   "source": [
    "net = NeuralNetwork(10, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8hq_2sgP_9k"
   },
   "source": [
    "In class you have seen that there are two main steps that are repeated over and over during the training process: the forward pass and backward pass. The forward is just making your input going through the network, doing the weights multiplication and so on as you have seen for the perceptron. While in Keras you have the fit() method that does both the step for you, in Pytorch, you have to implement the methods yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kP5BLVDIQhpN"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_hidden):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, num_hidden)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.linear2 = nn.Linear(num_hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        l1 = self.linear1(x)\n",
    "        activation = self.sigmoid(l1)\n",
    "        l2 = self.linear2(activation)\n",
    "        output = self.sigmoid(l2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NeK_lJYBQ_FL"
   },
   "source": [
    "As you can see in the code, each step in the forward pass is assigned to a variable, that is then passed to next layer!\n",
    "\n",
    "- The input x is passed in a linear layer\n",
    "- the output of that is assigned to the variable l1\n",
    "- l1 is activated using the sigmoid and assigned to the activation variable\n",
    "- the activation variable is passed to the second layer...\n",
    "...till the output that is returned!\n",
    "\n",
    "Let's create a sample to play with and make a forward pass calling the forward method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "P4nz_QzF-J-1"
   },
   "outputs": [],
   "source": [
    "sample = torch.from_numpy(np.array(np.random.rand(10), dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kvUBb-CH-n9_",
    "outputId": "20601a5c-7d92-4769-b383-7dc4e411c468"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tA9E26bRQ60U"
   },
   "outputs": [],
   "source": [
    "net = NeuralNetwork(10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xh5l6Nmd-c_8",
    "outputId": "e054b4d0-157f-47da-f9f3-a42cc8101f08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4554], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.forward(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbLWLKCkR6Co"
   },
   "source": [
    "As expected, you got a single value between 0 and 1 (the output of the sigmoid).\n",
    "Now that we have implemented the forward pass, we need to implement the backpropagation! Not from scratch, don't worry!\n",
    "\n",
    "First, we need to define both an optimizer and a loss function to use this training. I chose Adam as optimizer, since it is a common choice among the scientific community, and the Binary Cross Entropy loss since we are in the binary classification setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "agmgCwrLD15n",
    "outputId": "86478e80-fee7-4934-c9f4-d99a9e73a934"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 0.72\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_hidden):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, num_hidden)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.linear2 = nn.Linear(num_hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        l1 = self.linear1(x)\n",
    "        activation = self.sigmoid(l1)\n",
    "        l2 = self.linear2(activation)\n",
    "        output = self.sigmoid(l2)\n",
    "        return output\n",
    "        \n",
    "\n",
    "#torch_fit(x_tensor, y_true_tensor, model=model, loss=loss, lr=0.1, num_epochs=30)\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "model = NeuralNetwork(10, 5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "x = np.random.rand(1000,10)\n",
    "y = np.random.randint(0, 2, 1000)\n",
    "x_tensor = torch.tensor(x).float()\n",
    "y_true_tensor = torch.tensor(y).float()\n",
    "y_true_tensor = y_true_tensor.view(1000,1) # view function is the same as reshape in numpy\n",
    "y_pred_tensor = model(x_tensor)\n",
    "loss_value = loss(y_pred_tensor, y_true_tensor)\n",
    "print(f\"Initial loss: {loss_value.item():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2w8riJwWHnYt"
   },
   "source": [
    "and then wrap the training process in a unique function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uSo5PiJnHsdN",
    "outputId": "dfd616cb-8172-4e44-f60a-3daa27222abd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss 0.61\n",
      "Epoch 1, loss 0.61\n",
      "Epoch 2, loss 0.61\n",
      "Epoch 3, loss 0.61\n",
      "Epoch 4, loss 0.61\n",
      "Epoch 5, loss 0.61\n",
      "Epoch 6, loss 0.61\n",
      "Epoch 7, loss 0.61\n",
      "Epoch 8, loss 0.61\n",
      "Epoch 9, loss 0.61\n",
      "Epoch 10, loss 0.61\n",
      "Epoch 11, loss 0.61\n",
      "Epoch 12, loss 0.61\n",
      "Epoch 13, loss 0.60\n",
      "Epoch 14, loss 0.60\n",
      "Epoch 15, loss 0.60\n",
      "Epoch 16, loss 0.60\n",
      "Epoch 17, loss 0.60\n",
      "Epoch 18, loss 0.60\n",
      "Epoch 19, loss 0.60\n",
      "Epoch 20, loss 0.60\n",
      "Epoch 21, loss 0.60\n",
      "Epoch 22, loss 0.60\n",
      "Epoch 23, loss 0.60\n",
      "Epoch 24, loss 0.60\n",
      "Epoch 25, loss 0.60\n",
      "Epoch 26, loss 0.60\n",
      "Epoch 27, loss 0.60\n",
      "Epoch 28, loss 0.60\n",
      "Epoch 29, loss 0.60\n",
      "Epoch 30, loss 0.60\n",
      "Epoch 31, loss 0.60\n",
      "Epoch 32, loss 0.60\n",
      "Epoch 33, loss 0.59\n",
      "Epoch 34, loss 0.59\n",
      "Epoch 35, loss 0.59\n",
      "Epoch 36, loss 0.59\n",
      "Epoch 37, loss 0.59\n",
      "Epoch 38, loss 0.59\n",
      "Epoch 39, loss 0.59\n",
      "Epoch 40, loss 0.59\n",
      "Epoch 41, loss 0.59\n",
      "Epoch 42, loss 0.59\n",
      "Epoch 43, loss 0.59\n",
      "Epoch 44, loss 0.59\n",
      "Epoch 45, loss 0.59\n",
      "Epoch 46, loss 0.59\n",
      "Epoch 47, loss 0.59\n",
      "Epoch 48, loss 0.59\n",
      "Epoch 49, loss 0.58\n",
      "Epoch 50, loss 0.58\n",
      "Epoch 51, loss 0.58\n",
      "Epoch 52, loss 0.58\n",
      "Epoch 53, loss 0.58\n",
      "Epoch 54, loss 0.58\n",
      "Epoch 55, loss 0.58\n",
      "Epoch 56, loss 0.58\n",
      "Epoch 57, loss 0.58\n",
      "Epoch 58, loss 0.58\n",
      "Epoch 59, loss 0.58\n",
      "Epoch 60, loss 0.58\n",
      "Epoch 61, loss 0.58\n",
      "Epoch 62, loss 0.58\n",
      "Epoch 63, loss 0.58\n",
      "Epoch 64, loss 0.57\n",
      "Epoch 65, loss 0.57\n",
      "Epoch 66, loss 0.57\n",
      "Epoch 67, loss 0.57\n",
      "Epoch 68, loss 0.57\n",
      "Epoch 69, loss 0.57\n",
      "Epoch 70, loss 0.57\n",
      "Epoch 71, loss 0.57\n",
      "Epoch 72, loss 0.57\n",
      "Epoch 73, loss 0.57\n",
      "Epoch 74, loss 0.57\n",
      "Epoch 75, loss 0.57\n",
      "Epoch 76, loss 0.57\n",
      "Epoch 77, loss 0.57\n",
      "Epoch 78, loss 0.56\n",
      "Epoch 79, loss 0.56\n",
      "Epoch 80, loss 0.56\n",
      "Epoch 81, loss 0.56\n",
      "Epoch 82, loss 0.56\n",
      "Epoch 83, loss 0.56\n",
      "Epoch 84, loss 0.56\n",
      "Epoch 85, loss 0.56\n",
      "Epoch 86, loss 0.56\n",
      "Epoch 87, loss 0.56\n",
      "Epoch 88, loss 0.56\n",
      "Epoch 89, loss 0.56\n",
      "Epoch 90, loss 0.56\n",
      "Epoch 91, loss 0.55\n",
      "Epoch 92, loss 0.55\n",
      "Epoch 93, loss 0.55\n",
      "Epoch 94, loss 0.55\n",
      "Epoch 95, loss 0.55\n",
      "Epoch 96, loss 0.55\n",
      "Epoch 97, loss 0.55\n",
      "Epoch 98, loss 0.55\n",
      "Epoch 99, loss 0.55\n"
     ]
    }
   ],
   "source": [
    "def torch_fit(x, y, model, loss, lr, num_epochs):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred_tensor = model(x_tensor)\n",
    "        loss_value = loss(y_pred_tensor, y_true_tensor)\n",
    "        print(f'Epoch {epoch}, loss {loss_value.item():.2f}')\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "    return model\n",
    "\n",
    "model = torch_fit(x_tensor, y_true_tensor, model=model, loss=loss, lr=0.1, num_epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijMUN-7yI5NE"
   },
   "source": [
    "Nice! You have trained your first model in Pytorch!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Pytorch Intro.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
